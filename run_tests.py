#!/usr/bin/env python
"""
Script para ejecutar todos los tests del proyecto admin_events
Este script valida que la modernizaciÃ³n del sistema legado fue efectiva
"""

import os
import sys
import django
from django.conf import settings
from django.test.utils import get_runner
from django.core.management import execute_from_command_line

def setup_django():
    """Configurar Django para los tests"""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'admin_manage_events.settings')
    django.setup()

def run_all_tests():
    """Ejecutar todos los tests del proyecto"""
    print("=" * 80)
    print("EJECUTANDO TESTS DE VALIDACIÃ“N DE MODERNIZACIÃ“N")
    print("Proyecto: Admin Events - Sistema Legado")
    print("Objetivo: Validar que la modernizaciÃ³n fue efectiva")
    print("=" * 80)
    
    # Apps a testear
    test_apps = [
        'accounts.tests',
        'apps.attendees.tests', 
        'apps.events.tests',
        'apps.ticket_categories.tests',
        'apps.attachments.tests',
        'apps.inventory.tests',
    ]
    
    print("\nğŸ“‹ MÃ“DULOS A TESTEAR:")
    for app in test_apps:
        print(f"  âœ“ {app}")
    
    print("\nğŸš€ INICIANDO EJECUCIÃ“N DE TESTS...\n")
    
    # Ejecutar tests
    for app in test_apps:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª TESTING: {app}")
        print(f"{'='*60}")
        
        try:
            # Ejecutar tests para la app especÃ­fica
            execute_from_command_line([
                'manage.py', 'test', app, '--verbosity=2'
            ])
            print(f"âœ… {app}: TESTS COMPLETADOS")
            
        except SystemExit as e:
            if e.code == 0:
                print(f"âœ… {app}: TESTS COMPLETADOS EXITOSAMENTE")
            else:
                print(f"âŒ {app}: TESTS FALLARON (cÃ³digo: {e.code})")
        except Exception as e:
            print(f"ğŸ’¥ {app}: ERROR EJECUTANDO TESTS - {str(e)}")

def run_coverage_analysis():
    """Ejecutar anÃ¡lisis de cobertura de cÃ³digo"""
    print("\n" + "="*80)
    print("ğŸ“Š ANÃLISIS DE COBERTURA DE CÃ“DIGO")
    print("="*80)
    
    try:
        import coverage
        
        # Configurar coverage
        cov = coverage.Coverage(source=['.'])
        cov.start()
        
        # Ejecutar tests con coverage
        execute_from_command_line(['manage.py', 'test', '--verbosity=1'])
        
        cov.stop()
        cov.save()
        
        print("\nğŸ“ˆ REPORTE DE COBERTURA:")
        cov.report()
        
        # Generar reporte HTML
        cov.html_report(directory='htmlcov')
        print("\nğŸ“ Reporte HTML generado en: htmlcov/index.html")
        
    except ImportError:
        print("âš ï¸  Coverage no instalado. Instalar con: pip install coverage")
        print("ğŸ’¡ Ejecutando tests sin anÃ¡lisis de cobertura...")
        execute_from_command_line(['manage.py', 'test'])

def validate_modernization_metrics():
    """Validar mÃ©tricas de modernizaciÃ³n segÃºn el documento"""
    print("\n" + "="*80)
    print("ğŸ“Š VALIDACIÃ“N DE MÃ‰TRICAS DE MODERNIZACIÃ“N")
    print("="*80)
    
    metrics = {
        "cobertura_tests": "â‰¥70%",
        "aislamiento_dominio": "â‰¥95%", 
        "rendimiento_consultas": "<2s para paginaciÃ³n",
        "concurrencia": "Sin condiciones de carrera",
        "seguridad": "ValidaciÃ³n de datos sensibles",
        "escalabilidad": "<5s para 100 registros"
    }
    
    print("ğŸ¯ MÃ‰TRICAS OBJETIVO SEGÃšN DOCUMENTACIÃ“N:")
    for metric, target in metrics.items():
        print(f"  â€¢ {metric}: {target}")
    
    print("\nğŸ“‹ VALIDACIONES IMPLEMENTADAS EN TESTS:")
    validations = [
        "âœ“ Tests de rendimiento (PerformanceTests)",
        "âœ“ Tests de concurrencia (ConcurrencyTests)", 
        "âœ“ Tests de seguridad (SecurityTests)",
        "âœ“ Tests de integraciÃ³n (IntegrationTests)",
        "âœ“ Tests de migraciÃ³n (MigrationCompatibilityTests)",
        "âœ“ ValidaciÃ³n de estructura de datos para microservicio",
        "âœ“ Tests de aislamiento por empresa",
        "âœ“ ValidaciÃ³n de lÃ³gica de negocio crÃ­tica"
    ]
    
    for validation in validations:
        print(f"  {validation}")

def display_test_categories():
    """Mostrar categorÃ­as de tests implementados"""
    print("\n" + "="*80)
    print("ğŸ“š CATEGORÃAS DE TESTS IMPLEMENTADOS")
    print("="*80)
    
    categories = {
        "ğŸ—ï¸  TESTS FUNCIONALES": [
            "RF-001: Crear asistente con datos vÃ¡lidos",
            "RF-002: Crear compra vÃ¡lida", 
            "RF-003: ValidaciÃ³n de tickets agotados",
            "RF-004: Crear ticket vÃ¡lido",
            "RF-005: Flujo completo de registro",
            "RF-006: Asistente con archivo adjunto"
        ],
        "ğŸ”’ TESTS DE SEGURIDAD": [
            "SEC-001: ValidaciÃ³n de formato de email",
            "SEC-002: Integridad de datos del asistente", 
            "SEC-003: Datos sensibles no expuestos",
            "AUTH-001-005: Tests de autenticaciÃ³n",
            "PERM-001-004: Tests de permisos",
            "AUD-001-003: Tests de auditorÃ­a"
        ],
        "âš¡ TESTS DE RENDIMIENTO": [
            "RNF-001: Rendimiento en creaciÃ³n masiva",
            "RNF-002: Rendimiento de consultas con paginaciÃ³n",
            "RNF-003: CondiciÃ³n de carrera en compras",
            "PERF-001-002: Tests de rendimiento general"
        ],
        "ğŸ”— TESTS DE INTEGRACIÃ“N": [
            "INT-001: Flujo completo de compra para evento",
            "INT-002: ValidaciÃ³n de capacidad del evento", 
            "INT-003: Compra con mÃºltiples categorÃ­as",
            "RelaciÃ³n usuario-empresa-asistente"
        ],
        "ğŸš€ TESTS DE MIGRACIÃ“N": [
            "MIG-001: Compatibilidad de estructura de datos",
            "MIG-002: Formato de serializaciÃ³n para API REST",
            "MIG-003-004: Compatibilidad con microservicio"
        ],
        "ğŸ’¼ TESTS DE LÃ“GICA DE NEGOCIO": [
            "BL-001: CategorizaciÃ³n de eventos por fecha",
            "BL-002: Regla de negocio para eventos activos",
            "BL-003: Aislamiento de eventos por empresa"
        ]
    }
    
    for category, tests in categories.items():
        print(f"\n{category}:")
        for test in tests:
            print(f"  â€¢ {test}")

def main():
    """FunciÃ³n principal"""
    if len(sys.argv) > 1 and 'help' in sys.argv[1]:
        print("""
USO: python run_tests.py [opciÃ³n]

OPCIONES:
  help        - Mostrar esta ayuda
  coverage    - Ejecutar con anÃ¡lisis de cobertura
  metrics     - Solo mostrar mÃ©tricas sin ejecutar tests
  categories  - Solo mostrar categorÃ­as de tests
  
Sin argumentos: Ejecutar todos los tests bÃ¡sicos
        """)
        return
    
    # Cambiar al directorio del proyecto
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.join(script_dir, 'admin_manage_events')
    os.chdir(project_dir)
    
    # Configurar Django
    setup_django()
    
    # Mostrar informaciÃ³n inicial
    display_test_categories()
    validate_modernization_metrics()
    
    # Ejecutar segÃºn argumentos
    if len(sys.argv) > 1:
        if 'coverage' in sys.argv[1]:
            run_coverage_analysis()
        elif 'metrics' in sys.argv[1]:
            print("\nâœ… MÃ©tricas mostradas. No se ejecutaron tests.")
            return
        elif 'categories' in sys.argv[1]:
            print("\nâœ… CategorÃ­as mostradas. No se ejecutaron tests.")
            return
    else:
        run_all_tests()
    
    print("\n" + "="*80)
    print("ğŸ‰ VALIDACIÃ“N DE MODERNIZACIÃ“N COMPLETADA")
    print("="*80)
    print("ğŸ“„ Revisar el documento de modernizaciÃ³n en:")
    print("   modernization_doc_admin_events/input.md")
    print("ğŸ”— Comparar con la implementaciÃ³n del microservicio en:")
    print("   admin_events_upgrade/admin_events_attendees/")
    print("="*80)

if __name__ == '__main__':
    main()
